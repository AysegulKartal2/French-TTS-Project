{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "511T2cmrA7x2",
        "outputId": "7cae8e8b-e48e-4e14-ce73-fdab5a6febb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/tacotron2.git\n",
        "%cd tacotron2\n",
        "!git clone https://github.com/NVIDIA/apex\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33D-0_7kBDu1",
        "outputId": "ed7d2a39-92e4-4e7a-e51b-69c4a3d80ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tacotron2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Total 412 (delta 0), reused 0 (delta 0), pack-reused 412 (from 1)\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.70 MiB | 8.98 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n",
            "/content/tacotron2\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 12123, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 12123 (delta 27), reused 12 (delta 11), pack-reused 12052 (from 3)\u001b[K\n",
            "Receiving objects: 100% (12123/12123), 15.79 MiB | 27.23 MiB/s, done.\n",
            "Resolving deltas: 100% (8396/8396), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keithito/tacotron.git keithito_tacotron\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TPPH2oNBb14",
        "outputId": "ccdbc54f-b494-4dd5-fe66-997ff92c9207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'keithito_tacotron'...\n",
            "remote: Enumerating objects: 396, done.\u001b[K\n",
            "remote: Total 396 (delta 0), reused 0 (delta 0), pack-reused 396 (from 1)\u001b[K\n",
            "Receiving objects: 100% (396/396), 113.24 KiB | 5.66 MiB/s, done.\n",
            "Resolving deltas: 100% (233/233), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv keithito_tacotron/tacotron tacotron2/tacotron\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZVVVw-SBdbX",
        "outputId": "af58d93c-c77a-4b92-8d66-75128e8b2a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'keithito_tacotron/tacotron': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls keithito_tacotron\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Rh-MO1Bg-V",
        "outputId": "5ce7a04b-1da9-4e2e-ece4-5509291988af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets\thparams.py  preprocess.py     synthesizer.py  TRAINING_DATA.md\n",
            "demo_server.py\tLICENSE     README.md\t      tests\t      train.py\n",
            "eval.py\t\tmodels\t    requirements.txt  text\t      util\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p tacotron2/tacotron\n"
      ],
      "metadata": {
        "id": "a6-IlO0nBxXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp keithito_tacotron/*.py tacotron2/tacotron/\n"
      ],
      "metadata": {
        "id": "EI4V2U2IBy1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r keithito_tacotron/datasets tacotron2/tacotron/\n",
        "!cp -r keithito_tacotron/text tacotron2/tacotron/\n",
        "!cp -r keithito_tacotron/util tacotron2/tacotron/\n"
      ],
      "metadata": {
        "id": "Dx0RRO0cB1T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/tacotron2')\n"
      ],
      "metadata": {
        "id": "lNs9q40BB3r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/Tacotron2_Sounds/metadata.csv', header=None, names=['id', 'text'])\n",
        "\n",
        "# NaN değerleri içeren satırları at\n",
        "metadata = metadata.dropna(subset=['text'])\n",
        "\n",
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "    for index, row in metadata.iterrows():\n",
        "        wav_file = f\"{int(row['id'])}.wav\"  # id int tipine çevir\n",
        "        text = str(row['text']).strip()\n",
        "        f.write(f\"{wav_file}|{text}\\n\")\n",
        "\n",
        "print(\"train.txt dosyası oluşturuldu.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbvaYRs-Dx-T",
        "outputId": "2413d1f1-e015-4d3d-b396-db38ecdeb8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt dosyası oluşturuldu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_files = '/content/drive/MyDrive/Tacotron2_Sounds/train.txt'\n"
      ],
      "metadata": {
        "id": "UKaJp3zlELh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=output --log_directory=logs --hparams=\"training_files='/content/drive/MyDrive/Tacotron2_Sounds/train.txt',batch_size=32,epochs=100\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmG6Um3XEPbq",
        "outputId": "5786f185-31e9-419c-9d51-89842fb4f130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 14, in <module>\n",
            "    from data_utils import TextMelLoader, TextMelCollate\n",
            "  File \"/content/tacotron2/data_utils.py\", line 8, in <module>\n",
            "    from text import text_to_sequence\n",
            "  File \"/content/tacotron2/text/__init__.py\", line 3, in <module>\n",
            "    from text import cleaners\n",
            "  File \"/content/tacotron2/text/cleaners.py\", line 16, in <module>\n",
            "    from unidecode import unidecode\n",
            "ModuleNotFoundError: No module named 'unidecode'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==1.15\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2fHRc5iEpRZ",
        "outputId": "8f4e5e47-bd38-4cd8-e654-a1a62414fb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/tacotron2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHfIYO2DFT-e",
        "outputId": "021ed58e-96b6-459d-f77d-2ca3033d46cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tacotron2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf tacotron2\n",
        "!git clone https://github.com/NVIDIA/tacotron2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lekBAjUaF1YU",
        "outputId": "92283323-6617-41ae-ced9-bf1bfe80a159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tacotron2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 412 (delta 140), reused 126 (delta 126), pack-reused 240 (from 1)\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.68 MiB | 26.95 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r tacotron2/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aYTNSCSGIqK",
        "outputId": "8bdb3d8e-4e20-492f-e7b2-00ba05fb8287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==2.1.0 (from -r tacotron2/requirements.txt (line 1))\n",
            "  Downloading matplotlib-2.1.0.tar.gz (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.2 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat tacotron2/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6n1O5DuGnKu",
        "outputId": "bf0c5ffe-3d24-418b-b68f-f17fa13c471c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib==2.1.0\n",
            "tensorflow==1.15.2\n",
            "numpy==1.13.3\n",
            "inflect==0.2.5\n",
            "librosa==0.6.0\n",
            "scipy==1.0.0\n",
            "Unidecode==1.0.22\n",
            "pillow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dosyayı oku\n",
        "with open('tacotron2/requirements.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# tensorflow içeren satırı kaldır\n",
        "lines = [line for line in lines if 'tensorflow' not in line.lower()]\n",
        "\n",
        "# Dosyayı tekrar yaz\n",
        "with open('tacotron2/requirements.txt', 'w') as file:\n",
        "    file.writelines(lines)\n"
      ],
      "metadata": {
        "id": "6X3VtH_aGsfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements.txt içeriğini oku\n",
        "with open('tacotron2/requirements.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Eski ve sorun çıkaran sürüm sabitlemelerini temizle\n",
        "new_lines = []\n",
        "for line in lines:\n",
        "    # tensorflow zaten sorunlu, onu çıkar\n",
        "    if 'tensorflow' in line.lower():\n",
        "        continue\n",
        "    # matplotlib, numpy, librosa, scipy, scikit-learn sürümünü boş bırak veya kaldır\n",
        "    if any(pkg in line.lower() for pkg in ['matplotlib', 'numpy', 'librosa', 'scipy', 'scikit-learn']):\n",
        "        # sadece paket ismi kalsın, sürüm olmadan\n",
        "        pkg_name = line.split('==')[0]\n",
        "        new_lines.append(pkg_name + '\\n')\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "# Dosyayı güncelle\n",
        "with open('tacotron2/requirements.txt', 'w') as file:\n",
        "    file.writelines(new_lines)\n"
      ],
      "metadata": {
        "id": "1Yg-SVi0Ichz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r tacotron2/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXQFblDIedp",
        "outputId": "b9cc09db-e7ee-4845-a9e3-46b92dff5791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r tacotron2/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r tacotron2/requirements.txt (line 2)) (2.0.2)\n",
            "Collecting inflect==0.2.5 (from -r tacotron2/requirements.txt (line 3))\n",
            "  Using cached inflect-0.2.5-py2.py3-none-any.whl.metadata (50 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from -r tacotron2/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r tacotron2/requirements.txt (line 5)) (1.15.3)\n",
            "Collecting Unidecode==1.0.22 (from -r tacotron2/requirements.txt (line 6))\n",
            "  Using cached Unidecode-1.0.22-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r tacotron2/requirements.txt (line 7)) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r tacotron2/requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r tacotron2/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->-r tacotron2/requirements.txt (line 4)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->-r tacotron2/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->-r tacotron2/requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->-r tacotron2/requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r tacotron2/requirements.txt (line 4)) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->-r tacotron2/requirements.txt (line 4)) (2025.4.26)\n",
            "Downloading inflect-0.2.5-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.0.22-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode, inflect\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 7.5.0\n",
            "    Uninstalling inflect-7.5.0:\n",
            "      Successfully uninstalled inflect-7.5.0\n",
            "Successfully installed Unidecode-1.0.22 inflect-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ses dosularının bulunduğu klasör\n",
        "data_path = \"/content/drive/MyDrive/Tacotron2_Sounds\"\n",
        "\n",
        "# Tacotron2 klasörü altına wavs symlink'i oluştur\n",
        "if not os.path.exists(\"/content/tacotron2/wavs\"):\n",
        "    os.symlink(data_path, \"/content/tacotron2/wavs\")\n",
        "\n",
        "# train.txt'yi de tacotron2 klasörüne kopyala\n",
        "!cp /content/drive/MyDrive/Tacotron2_Sounds/train.txt /content/tacotron2/train.txt\n"
      ],
      "metadata": {
        "id": "pNuERwco6Kix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaynak dosya (örneğin metadata.txt)\n",
        "source_path = \"/content/drive/MyDrive/Tacotron2_Sounds/metadata.csv\"\n",
        "\n",
        "# Hedef dosya (train.txt)\n",
        "target_path = \"/content/tacotron2/train.txt\"\n",
        "\n",
        "with open(source_path, 'r', encoding='utf-8') as infile, open(target_path, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) == 2:\n",
        "            wav_name, text = parts\n",
        "            outfile.write(f\"wavs/{wav_name}.wav|{text}\\n\")\n",
        "\n",
        "print(\"✅ train.txt dosyası başarıyla oluşturuldu.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpoaFrlU-ivv",
        "outputId": "b0b6675d-26c3-4e96-e7c9-1f66b369e0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train.txt dosyası başarıyla oluşturuldu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/tacotron2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJI01bi_awd",
        "outputId": "41be1b36-1341-4836-b7e8-f86702811ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t\t     inference.ipynb\tmultiproc.py\t   text\n",
            "audio_processing.py  keithito_tacotron\tplotting_utils.py  train.py\n",
            "data_utils.py\t     layers.py\t\t__pycache__\t   train.txt\n",
            "demo.wav\t     LICENSE\t\tREADME.md\t   utils.py\n",
            "distributed.py\t     logger.py\t\trequirements.txt   waveglow\n",
            "Dockerfile\t     loss_function.py\tstft.py\t\t   wavs\n",
            "filelists\t     loss_scaler.py\ttacotron2\n",
            "hparams.py\t     model.py\t\ttensorboard.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWFL_UGSA-Ed",
        "outputId": "132e9d0c-eaae-4c92-cd3f-7963738efe52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 17, in <module>\n",
            "    from hparams import create_hparams\n",
            "  File \"/content/tacotron2/hparams.py\", line 1, in <module>\n",
            "    import tensorflow as tf\n",
            "ModuleNotFoundError: No module named 'tensorflow'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/tacotron2\n"
      ],
      "metadata": {
        "id": "1p-bhnuvBwvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8vRbkBZCDx8",
        "outputId": "36e3381c-8fbd-4403-b7f7-0cf910884e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/tacotron2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEOGrYnpCFIH",
        "outputId": "10fd0727-28bc-4c34-e257-f9527ac043d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tacotron2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 412 (delta 140), reused 126 (delta 126), pack-reused 240 (from 1)\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.68 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd tacotron2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9FIDoUwCIqc",
        "outputId": "4fd3e69c-a15d-483e-d24c-06c6d85affba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXrJ-bZICMsf",
        "outputId": "3c2b0e8f-0935-4f1b-9fd5-9cd033a77f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Using cached matplotlib-2.1.0.tar.gz (35.7 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.2 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NVIDIA/tacotron2/main/requirements.txt -O requirements.txt\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkuduDxLCikT",
        "outputId": "ea596427-b4ce-441b-a93d-05d1a735bff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 18:17:48--  https://raw.githubusercontent.com/NVIDIA/tacotron2/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-18 18:17:48 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy scipy matplotlib pillow\n",
        "!pip install inflect librosa Unidecode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLCltDnKCtqu",
        "outputId": "f743a7ef-a7ee-438e-cd50-3dea93620b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.11/dist-packages (1.0.22)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Tacotron2_Sounds\"\n",
        "\n",
        "if not os.path.exists(\"wavs\"):\n",
        "    os.symlink(data_path, \"wavs\")\n"
      ],
      "metadata": {
        "id": "d2wuALYhDT4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Tacotron2_Sounds/metadata.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(df.columns)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JDokCREKxl",
        "outputId": "059968bb-7ea1-44c0-c5cb-36db7a28c003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['1|Merhaba. benim adım Ayşegül'], dtype='object')\n",
            "   1|Merhaba. benim adım Ayşegül\n",
            "0     2|Size kendimi tanıtacağım\n",
            "1            3|Ben 22 yaşındayım\n",
            "2         4|Isparta da yaşıyorum\n",
            "3       5|Kedileri çok seviyorum\n",
            "4  6|Film izlemeyi çok seviyorum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Tacotron2_Sounds/metadata.csv .\n"
      ],
      "metadata": {
        "id": "MQ2o6yBpEY_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRIgDm8tEbBX",
        "outputId": "0eb1668b-67e0-4076-b2a0-416236c6c96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 17, in <module>\n",
            "    from hparams import create_hparams\n",
            "  File \"/content/tacotron2/hparams.py\", line 1, in <module>\n",
            "    import tensorflow as tf\n",
            "ModuleNotFoundError: No module named 'tensorflow'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/DeepLearningExamples.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGJ2KfuwErap",
        "outputId": "dee36aa9-a8d6-4e45-e7b0-f96818333568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLearningExamples'...\n",
            "remote: Enumerating objects: 33828, done.\u001b[K\n",
            "remote: Total 33828 (delta 0), reused 0 (delta 0), pack-reused 33828 (from 1)\u001b[K\n",
            "Receiving objects: 100% (33828/33828), 110.22 MiB | 23.80 MiB/s, done.\n",
            "Resolving deltas: 100% (23838/23838), done.\n",
            "Updating files: 100% (5403/5403), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DeepLearningExamples/PyTorch/SpeechSynthesis/Tacotron2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaZJsx9XEukX",
        "outputId": "b40fca80-41c3-4247-f7ea-279644ebfa70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2/DeepLearningExamples/PyTorch/SpeechSynthesis/Tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJkKXbcFaQJ",
        "outputId": "b80b927b-a89a-4ead-cdce-774b76cf0560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4r2Dr8EFcbG",
        "outputId": "92484dc8-758f-4cce-f7b9-d9723613738c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tacotron2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYd6F5PVFo-3",
        "outputId": "b064bb07-e06e-4fe6-9a70-6b6521d79370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I7QrjF6Fqka",
        "outputId": "3b4c7ff1-c988-4c74-90cc-ec7239236908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio_processing.py   hparams.py\tmetadata.csv\t   stft.py\n",
            "data_utils.py\t      inference.ipynb\tmodel.py\t   tensorboard.png\n",
            "DeepLearningExamples  layers.py\t\tmultiproc.py\t   text\n",
            "demo.wav\t      LICENSE\t\tplotting_utils.py  train.py\n",
            "distributed.py\t      logger.py\t\t__pycache__\t   utils.py\n",
            "Dockerfile\t      loss_function.py\tREADME.md\t   waveglow\n",
            "filelists\t      loss_scaler.py\trequirements.txt   wavs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/tacotron2/hparams.py\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "new_lines = []\n",
        "for line in lines:\n",
        "    if \"import tensorflow as tf\" in line:\n",
        "        continue  # Bu satırı atla\n",
        "    if \"tf.contrib.training.HParams\" in line:\n",
        "        line = line.replace(\"tf.contrib.training.HParams\", \"HParams\")\n",
        "    new_lines.append(line)\n",
        "\n",
        "# En üste from utils import HParams ekle\n",
        "new_lines.insert(0, \"from utils import HParams\\n\")\n",
        "\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(new_lines)\n",
        "\n",
        "print(\"hparams.py dosyası başarıyla güncellendi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYKGpCD_GSln",
        "outputId": "ac5214f2-6b33-469d-dd0c-787f8623fef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hparams.py dosyası başarıyla güncellendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparams_code = \"\"\"\n",
        "class HParams:\n",
        "    def __init__(self, **kwargs):\n",
        "        self._dict = kwargs\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    def parse(self, hparams_string):\n",
        "        for pair in hparams_string.split(\",\"):\n",
        "            key, value = pair.split(\"=\")\n",
        "            key, value = key.strip(), value.strip()\n",
        "            try:\n",
        "                # Try to interpret as Python literal\n",
        "                value = eval(value)\n",
        "            except:\n",
        "                pass\n",
        "            setattr(self, key, value)\n",
        "            self._dict[key] = value\n",
        "\n",
        "    def values(self):\n",
        "        return self._dict\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/tacotron2/utils.py\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write(hparams_code)\n",
        "\n",
        "print(\"✅ HParams sınıfı utils.py dosyasına eklendi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnNIST2QGiuy",
        "outputId": "9b1d3b81-312f-46e9-80d6-faa182d0d74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ HParams sınıfı utils.py dosyasına eklendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xtnJSdQGm3T",
        "outputId": "1b6ee65b-f1ad-48dc-fc58-6be3c887dc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 278, in <module>\n",
            "    hparams = create_hparams(args.hparams)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/tacotron2/hparams.py\", line 89, in create_hparams\n",
            "    tf.logging.info('Parsing command line hparams: %s', hparams_string)\n",
            "    ^^\n",
            "NameError: name 'tf' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.logging satırlarını kaldır ve import tensorflow as tf satırını sil\n",
        "path = \"/content/tacotron2/hparams.py\"\n",
        "\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in lines:\n",
        "        if \"import tensorflow\" in line:\n",
        "            continue\n",
        "        if \"tf.logging\" in line:\n",
        "            continue\n",
        "        f.write(line)\n",
        "\n",
        "print(\"✅ TensorFlow referansları hparams.py dosyasından temizlendi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igGaio0MG3IE",
        "outputId": "24bc43e8-83e6-4991-fd09-2bca9bbb4aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TensorFlow referansları hparams.py dosyasından temizlendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwM8igERG5sF",
        "outputId": "81972c92-cebd-43cd-dacb-3b7c35ecbc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 17, in <module>\n",
            "    from hparams import create_hparams\n",
            "  File \"/content/tacotron2/hparams.py\", line 93\n",
            "    return hparams\n",
            "    ^\n",
            "IndentationError: expected an indented block after 'if' statement on line 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_lines = []\n",
        "with open(\"/content/tacotron2/hparams.py\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    if \"if hparams_string:\" in line:\n",
        "        corrected_lines.append(line)\n",
        "        corrected_lines.append(\"    hparams.parse(hparams_string)\\n\")\n",
        "    elif \"if verbose:\" in line:\n",
        "        corrected_lines.append(line)\n",
        "        corrected_lines.append(\"    print('Final parsed hparams:', hparams.values())\\n\")\n",
        "    elif \"return hparams\" in line and not line.startswith(\"    \"):\n",
        "        corrected_lines.append(\"    \" + line)  # Girinti ekle\n",
        "    else:\n",
        "        corrected_lines.append(line)\n",
        "\n",
        "with open(\"/content/tacotron2/hparams.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(corrected_lines)\n",
        "\n",
        "print(\"✅ Girinti hatası düzeltildi ve if blokları tamamlandı.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvB6gWPDHGwB",
        "outputId": "b83acf6b-16f7-4600-d526-efaa8f18bcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Girinti hatası düzeltildi ve if blokları tamamlandı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EIPKwhmHIF-",
        "outputId": "df8c0c3e-1946-40d4-e12b-936846062a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 17, in <module>\n",
            "    from hparams import create_hparams\n",
            "  File \"/content/tacotron2/hparams.py\", line 89\n",
            "    hparams.parse(hparams_string)\n",
            "    ^\n",
            "IndentationError: expected an indented block after 'if' statement on line 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "with open(\"/content/tacotron2/hparams.py\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        lines.append(line)\n",
        "\n",
        "new_lines = []\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == \"if hparams_string:\":\n",
        "        new_lines.append(line)\n",
        "        new_lines.append(\"    hparams.parse(hparams_string)\\n\")\n",
        "    elif line.strip() == \"if verbose:\":\n",
        "        new_lines.append(line)\n",
        "        new_lines.append(\"    print('Final parsed hparams:', hparams.values())\\n\")\n",
        "    elif line.strip() == \"return hparams\":\n",
        "        new_lines.append(\"    return hparams\\n\")  # İçeride olacak şekilde girinti veriyoruz\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "with open(\"/content/tacotron2/hparams.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(new_lines)\n",
        "\n",
        "print(\"✅ hparams.py dosyasındaki girinti hatası düzeltildi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTZmF6pmHUSC",
        "outputId": "aaee5ab9-f4cd-449a-d77a-3fb9ff0f3159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ hparams.py dosyasındaki girinti hatası düzeltildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV dosyasını oku\n",
        "df = pd.read_csv('/content/tacotron2/metadata.csv', header=None)\n",
        "\n",
        "# Her satırı ayır (1|Metin)\n",
        "split_data = df[0].str.split('|', n=1, expand=True)\n",
        "\n",
        "# WAV dosya adlarını ve metinleri ayıkla\n",
        "split_data[0] = split_data[0].str.zfill(3)  # 001, 002...\n",
        "split_data[0] = 'wavs/' + split_data[0] + '.wav'  # wavs/001.wav\n",
        "\n",
        "# train.txt dosyasını yaz\n",
        "with open('/content/tacotron2/train.txt', 'w', encoding='utf-8') as f:\n",
        "    for i in range(len(split_data)):\n",
        "        f.write(f\"{split_data.iloc[i, 0]}|{split_data.iloc[i, 1]}\\n\")\n",
        "\n",
        "print(\"✅ train.txt dosyası oluşturuldu.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh9ZmtXlI3vR",
        "outputId": "7ee8c2af-f068-4dcb-adb9-c96fde0b5408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train.txt dosyası oluşturuldu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --output_directory=outdir --log_directory=logdir --hparams=\"training_files=train.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4x5NYW1I4yG",
        "outputId": "33508c9f-ff9b-4f57-eea0-0acfeb9629dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tacotron2/train.py\", line 17, in <module>\n",
            "    from hparams import create_hparams\n",
            "  File \"/content/tacotron2/hparams.py\", line 89\n",
            "    hparams.parse(hparams_string)\n",
            "    ^\n",
            "IndentationError: expected an indented block after 'if' statement on line 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "metadata_path = 'metadata.csv'\n",
        "train_txt_path = 'train.txt'\n",
        "\n",
        "with open(metadata_path, 'r', encoding='utf-8') as csvfile, \\\n",
        "     open(train_txt_path, 'w', encoding='utf-8') as trainfile:\n",
        "\n",
        "    reader = csv.reader(csvfile, delimiter='|')\n",
        "    for row in reader:\n",
        "        if len(row) < 2:\n",
        "            continue\n",
        "        num = row[0].strip()\n",
        "        text = row[1].strip()\n",
        "        # Ses dosyası yolu örneği: wavs/1.wav\n",
        "        wav_path = f\"wavs/{num}.wav\"\n",
        "        trainfile.write(f\"{wav_path}|{text}\\n\")\n"
      ],
      "metadata": {
        "id": "dWwmXqREMi7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l wavs/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFgcUuQsMx43",
        "outputId": "09df439d-1fac-49f8-d975-fd69f66f4404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15757\n",
            "-rw------- 1 root root 143080 May 14 13:36 100.wav\n",
            "-rw------- 1 root root 122382 May 14 13:36 101.wav\n",
            "-rw------- 1 root root 122382 May 14 13:37 102.wav\n",
            "-rw------- 1 root root 122382 May 14 13:37 103.wav\n",
            "-rw------- 1 root root 115798 May 14 13:37 104.wav\n",
            "-rw------- 1 root root 131790 May 14 14:06 105.wav\n",
            "-rw------- 1 root root 112034 May 14 14:06 106.wav\n",
            "-rw------- 1 root root 134614 May 14 14:06 107.wav\n",
            "-rw------- 1 root root 112974 May 14 14:06 108.wav\n",
            "-rw------- 1 root root 132732 May 14 14:07 109.wav\n",
            "-rw------- 1 root root 176008 May 14 11:37 10.wav\n",
            "-rw------- 1 root root 133672 May 14 14:07 110.wav\n",
            "-rw------- 1 root root 142140 May 14 14:07 111.wav\n",
            "-rw------- 1 root root 128028 May 14 14:07 112.wav\n",
            "-rw------- 1 root root 153430 May 14 14:07 113.wav\n",
            "-rw------- 1 root root 128968 May 14 14:08 114.wav\n",
            "-rw------- 1 root root 141198 May 14 14:08 115.wav\n",
            "-rw------- 1 root root 160014 May 14 14:08 116.wav\n",
            "-rw------- 1 root root 132732 May 14 14:09 117.wav\n",
            "-rw------- 1 root root 140258 May 14 14:09 118.wav\n",
            "-rw------- 1 root root 112974 May 14 14:10 119.wav\n",
            "-rw------- 1 root root 148726 May 14 11:39 11.wav\n",
            "-rw------- 1 root root 122382 May 14 14:10 120.wav\n",
            "-rw------- 1 root root 159074 May 14 11:39 12.wav\n",
            "-rw------- 1 root root 165660 May 14 12:20 13.wav\n",
            "-rw------- 1 root root 192002 May 14 12:20 14.wav\n",
            "-rw------- 1 root root 118620 May 14 12:20 15.wav\n",
            "-rw------- 1 root root 135554 May 14 12:20 16.wav\n",
            "-rw------- 1 root root 147784 May 14 12:21 17.wav\n",
            "-rw------- 1 root root 123324 May 14 12:21 18.wav\n",
            "-rw------- 1 root root 129910 May 14 12:21 19.wav\n",
            "-rw------- 1 root root 156252 May 14 10:01 1.wav\n",
            "-rw------- 1 root root 158134 May 14 12:21 20.wav\n",
            "-rw------- 1 root root 158134 May 14 12:21 21.wav\n",
            "-rw------- 1 root root 113916 May 14 12:22 22.wav\n",
            "-rw------- 1 root root 116738 May 14 12:22 23.wav\n",
            "-rw------- 1 root root 116738 May 14 12:22 24.wav\n",
            "-rw------- 1 root root 120502 May 14 12:23 25.wav\n",
            "-rw------- 1 root root 110152 May 14 12:23 26.wav\n",
            "-rw------- 1 root root 131790 May 14 12:23 27.wav\n",
            "-rw------- 1 root root 153430 May 14 12:24 28.wav\n",
            "-rw------- 1 root root 116738 May 14 12:24 29.wav\n",
            "-rw------- 1 root root 111094 May 14 10:02 2.wav\n",
            "-rw------- 1 root root 128028 May 14 12:24 30.wav\n",
            "-rw------- 1 root root 136494 May 14 12:25 31.wav\n",
            "-rw------- 1 root root 115798 May 14 12:25 32.wav\n",
            "-rw------- 1 root root 134614 May 14 12:25 33.wav\n",
            "-rw------- 1 root root 154370 May 14 12:26 34.wav\n",
            "-rw------- 1 root root 118620 May 14 12:26 35.wav\n",
            "-rw------- 1 root root 120502 May 14 12:26 36.wav\n",
            "-rw------- 1 root root 132732 May 14 12:27 37.wav\n",
            "-rw------- 1 root root 144022 May 14 12:27 38.wav\n",
            "-rw------- 1 root root 134614 May 14 12:27 39.wav\n",
            "-rw------- 1 root root 149666 May 14 10:02 3.wav\n",
            "-rw------- 1 root root 141198 May 14 12:28 40.wav\n",
            "-rw------- 1 root root 144022 May 14 12:28 41.wav\n",
            "-rw------- 1 root root 161896 May 14 12:29 42.wav\n",
            "-rw------- 1 root root 162838 May 14 12:29 43.wav\n",
            "-rw------- 1 root root 155310 May 14 12:29 44.wav\n",
            "-rw------- 1 root root 141198 May 14 12:30 45.wav\n",
            "-rw------- 1 root root 152488 May 14 12:30 46.wav\n",
            "-rw------- 1 root root 160014 May 14 12:30 47.wav\n",
            "-rw------- 1 root root 124264 May 14 12:30 48.wav\n",
            "-rw------- 1 root root 123324 May 14 12:31 49.wav\n",
            "-rw------- 1 root root 144962 May 14 10:02 4.wav\n",
            "-rw------- 1 root root 128968 May 14 12:31 50.wav\n",
            "-rw------- 1 root root 136494 May 14 12:31 51.wav\n",
            "-rw------- 1 root root 120502 May 14 13:14 52.wav\n",
            "-rw------- 1 root root 126146 May 14 13:14 53.wav\n",
            "-rw------- 1 root root 150606 May 14 13:14 54.wav\n",
            "-rw------- 1 root root 129910 May 14 13:15 55.wav\n",
            "-rw------- 1 root root 139318 May 14 13:15 56.wav\n",
            "-rw------- 1 root root 112974 May 14 13:18 57.wav\n",
            "-rw------- 1 root root 133672 May 14 13:19 58.wav\n",
            "-rw------- 1 root root 114856 May 14 13:18 59.wav\n",
            "-rw------- 1 root root 141198 May 14 10:02 5.wav\n",
            "-rw------- 1 root root 135554 May 14 13:19 60.wav\n",
            "-rw------- 1 root root 128968 May 14 13:19 61.wav\n",
            "-rw------- 1 root root 123324 May 14 13:20 62.wav\n",
            "-rw------- 1 root root 147784 May 14 13:21 63.wav\n",
            "-rw------- 1 root root 112034 May 14 13:21 64.wav\n",
            "-rw------- 1 root root 138376 May 14 13:21 65.wav\n",
            "-rw------- 1 root root  96040 May 14 13:22 66.wav\n",
            "-rw------- 1 root root 125206 May 14 13:22 67.wav\n",
            "-rw------- 1 root root 128968 May 14 13:22 68.wav\n",
            "-rw------- 1 root root 157192 May 14 13:25 69.wav\n",
            "-rw------- 1 root root 165660 May 14 14:15 6.wav\n",
            "-rw------- 1 root root 125206 May 14 13:25 70.wav\n",
            "-rw------- 1 root root 129910 May 14 13:25 71.wav\n",
            "-rw------- 1 root root 119560 May 14 13:26 72.wav\n",
            "-rw------- 1 root root 154370 May 14 13:26 73.wav\n",
            "-rw------- 1 root root 120502 May 14 13:26 74.wav\n",
            "-rw------- 1 root root 120502 May 14 13:26 75.wav\n",
            "-rw------- 1 root root 108270 May 14 13:27 76.wav\n",
            "-rw------- 1 root root 116738 May 14 13:27 77.wav\n",
            "-rw------- 1 root root  81928 May 14 13:27 78.wav\n",
            "-rw------- 1 root root 125206 May 14 13:27 79.wav\n",
            "-rw------- 1 root root 160014 May 14 14:15 7.wav\n",
            "-rw------- 1 root root 150606 May 14 13:27 80.wav\n",
            "-rw------- 1 root root 111094 May 14 13:28 81.wav\n",
            "-rw------- 1 root root 143080 May 14 13:28 82.wav\n",
            "-rw------- 1 root root 150606 May 14 13:28 83.wav\n",
            "-rw------- 1 root root 120502 May 14 13:28 84.wav\n",
            "-rw------- 1 root root 105448 May 14 13:28 85.wav\n",
            "-rw------- 1 root root 118620 May 14 13:28 86.wav\n",
            "-rw------- 1 root root 125206 May 14 13:30 87.wav\n",
            "-rw------- 1 root root 164718 May 14 13:30 88.wav\n",
            "-rw------- 1 root root 128028 May 14 13:30 89.wav\n",
            "-rw------- 1 root root 208936 May 14 11:36 8.wav\n",
            "-rw------- 1 root root 128028 May 14 13:32 90.wav\n",
            "-rw------- 1 root root 102626 May 14 13:32 91.wav\n",
            "-rw------- 1 root root 105448 May 14 13:32 92.wav\n",
            "-rw------- 1 root root 128028 May 14 13:33 93.wav\n",
            "-rw------- 1 root root 104508 May 14 13:33 94.wav\n",
            "-rw------- 1 root root 111094 May 14 13:34 95.wav\n",
            "-rw------- 1 root root 156252 May 14 13:35 96.wav\n",
            "-rw------- 1 root root 144962 May 14 13:35 97.wav\n",
            "-rw------- 1 root root 126146 May 14 13:35 98.wav\n",
            "-rw------- 1 root root 137436 May 14 13:36 99.wav\n",
            "-rw------- 1 root root 162838 May 14 11:36 9.wav\n",
            "-rw------- 1 root root   3485 May 17 20:57 metadata.csv\n",
            "drwx------ 2 root root   4096 May 16 20:47 tacotron2\n",
            "-rw------- 1 root root   3962 May 18 16:30 train_corrected.txt\n",
            "-rw------- 1 root root   7850 May 18 16:43 train_fullpath.txt\n",
            "-rw------- 1 root root   3485 May 18 16:28 train.txt\n",
            "-rw------- 1 root root    912 May 18 16:43 val_fullpath.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 train.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "365DmLMONShT",
        "outputId": "6bfcf4a0-1eb2-4c17-ee6a-610f89a9fdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wavs/1.wav|Merhaba. benim adım Ayşegül\n",
            "wavs/2.wav|Size kendimi tanıtacağım\n",
            "wavs/3.wav|Ben 22 yaşındayım\n",
            "wavs/4.wav|Isparta da yaşıyorum\n",
            "wavs/5.wav|Kedileri çok seviyorum\n",
            "wavs/6.wav|Film izlemeyi çok seviyorum\n",
            "wavs/7.wav|Kitap okumak beni rahatlatır\n",
            "wavs/8.wav|Spor yapmayı alışkanlık haline getirdim\n",
            "wavs/9.wav|Müzik dinlemeyi seviyorum\n",
            "wavs/10.wav|Yeni bir dil öğrenmek istiyorum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJJt5JoxOWBD",
        "outputId": "87eaf7cb-00a6-41ef-b8da-64a5b5c8a857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# metadata.csv dosyasının yolu (Drive içindeki)\n",
        "metadata_path = '/content/drive/MyDrive/Tacotron2_Sounds/metadata.csv'\n",
        "\n",
        "# train.txt dosyasını kaydedeceğin klasör\n",
        "save_path = '/content/drive/MyDrive/Tacotron2_Sounds/train.txt'\n",
        "\n",
        "# metadata dosyasını oku\n",
        "df = pd.read_csv(metadata_path, sep='|', header=None, names=['wav_path', 'text'])\n",
        "\n",
        "# Örneğin şu şekilde gözüküyorsa:\n",
        "# wav_path: wavs/91.wav\n",
        "# text: Sandalyeyi çektim\n",
        "\n",
        "# Dosyaya yaz\n",
        "with open(save_path, 'w', encoding='utf-8') as f:\n",
        "    for idx, row in df.iterrows():\n",
        "        line = f\"{row['wav_path']}|{row['text']}\\n\"\n",
        "        f.write(line)\n",
        "\n",
        "print(f'train.txt dosyası başarıyla oluşturuldu ve {save_path} içine kaydedildi.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPe-VaczOYBP",
        "outputId": "5c6fdcb9-55c7-4fdb-812e-3bed5d8a74f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt dosyası başarıyla oluşturuldu ve /content/drive/MyDrive/Tacotron2_Sounds/train.txt içine kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Drive'daki mevcut train.txt yolun\n",
        "train_txt_path = '/content/drive/MyDrive/Tacotron2_Sounds/train.txt'\n",
        "\n",
        "# Düzenlenmiş dosyanın kaydedileceği yer\n",
        "corrected_train_txt_path = '/content/drive/MyDrive/Tacotron2_Sounds/train_corrected.txt'\n",
        "\n",
        "lines = []\n",
        "with open(train_txt_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        idx, text = line.strip().split('|', 1)  # 1'e böl, sadece ilk '|' ile bölünsün\n",
        "        wav_file = f'{idx}.wav'  # Ses dosyasının adı\n",
        "        lines.append(f\"{wav_file}|{text}\\n\")\n",
        "\n",
        "with open(corrected_train_txt_path, 'w', encoding='utf-8') as f:\n",
        "    f.writelines(lines)\n",
        "\n",
        "print(f\"Düzeltilmiş train dosyası oluşturuldu: {corrected_train_txt_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU7TIYtqOx0b",
        "outputId": "aa071be9-4376-4617-bd2b-af1c174b90a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Düzeltilmiş train dosyası oluşturuldu: /content/drive/MyDrive/Tacotron2_Sounds/train_corrected.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Tacotron2_Sounds/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWDPRlSIQHSx",
        "outputId": "89cbb119-2eda-4f4f-80dd-fec9eca9f2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.wav  115.wav  20.wav  35.wav  4.wav   64.wav  79.wav  93.wav\n",
            "101.wav  116.wav  21.wav  36.wav  50.wav  65.wav  7.wav   94.wav\n",
            "102.wav  117.wav  22.wav  37.wav  51.wav  66.wav  80.wav  95.wav\n",
            "103.wav  118.wav  23.wav  38.wav  52.wav  67.wav  81.wav  96.wav\n",
            "104.wav  119.wav  24.wav  39.wav  53.wav  68.wav  82.wav  97.wav\n",
            "105.wav  11.wav   25.wav  3.wav   54.wav  69.wav  83.wav  98.wav\n",
            "106.wav  120.wav  26.wav  40.wav  55.wav  6.wav   84.wav  99.wav\n",
            "107.wav  12.wav   27.wav  41.wav  56.wav  70.wav  85.wav  9.wav\n",
            "108.wav  13.wav   28.wav  42.wav  57.wav  71.wav  86.wav  metadata.csv\n",
            "109.wav  14.wav   29.wav  43.wav  58.wav  72.wav  87.wav  tacotron2\n",
            "10.wav\t 15.wav   2.wav   44.wav  59.wav  73.wav  88.wav  train_corrected.txt\n",
            "110.wav  16.wav   30.wav  45.wav  5.wav   74.wav  89.wav  train_fullpath.txt\n",
            "111.wav  17.wav   31.wav  46.wav  60.wav  75.wav  8.wav   train.txt\n",
            "112.wav  18.wav   32.wav  47.wav  61.wav  76.wav  90.wav  val_fullpath.txt\n",
            "113.wav  19.wav   33.wav  48.wav  62.wav  77.wav  91.wav\n",
            "114.wav  1.wav\t  34.wav  49.wav  63.wav  78.wav  92.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/drive/MyDrive/Tacotron2_Sounds/train_corrected.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV1wPBvXQRjM",
        "outputId": "916f13b3-3d79-4e27-fa23-340c806169d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.wav|Merhaba. benim adım Ayşegül\n",
            "2.wav|Size kendimi tanıtacağım\n",
            "3.wav|Ben 22 yaşındayım\n",
            "4.wav|Isparta da yaşıyorum\n",
            "5.wav|Kedileri çok seviyorum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/drive/MyDrive/Tacotron2_Sounds/train_corrected.txt\"\n",
        "output_file = \"/content/drive/MyDrive/Tacotron2_Sounds/train_fullpath.txt\"\n",
        "folder_path = \"/content/drive/MyDrive/Tacotron2_Sounds/\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for line in f_in:\n",
        "        wav, text = line.strip().split(\"|\")\n",
        "        full_line = folder_path + wav + \"|\" + text\n",
        "        f_out.write(full_line + \"\\n\")\n"
      ],
      "metadata": {
        "id": "8ZUt7JbIQo20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/drive/MyDrive/Tacotron2_Sounds/train_corrected.txt\"\n",
        "output_train = \"/content/drive/MyDrive/Tacotron2_Sounds/train_fullpath.txt\"\n",
        "output_val = \"/content/drive/MyDrive/Tacotron2_Sounds/val_fullpath.txt\"\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Tacotron2_Sounds/\"\n",
        "\n",
        "# Dosyayı oku\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# %10 validasyon için ayırıyoruz\n",
        "val_size = int(len(lines) * 0.1)\n",
        "train_lines = lines[val_size:]\n",
        "val_lines = lines[:val_size]\n",
        "\n",
        "# Her satıra tam yol ekle\n",
        "def add_full_path(lines):\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(\"|\")\n",
        "        filename = parts[0]\n",
        "        text = parts[1]\n",
        "        full_path = base_path + filename\n",
        "        new_lines.append(full_path + \"|\" + text + \"\\n\")\n",
        "    return new_lines\n",
        "\n",
        "train_lines_full = add_full_path(train_lines)\n",
        "val_lines_full = add_full_path(val_lines)\n",
        "\n",
        "# Dosyalara yaz\n",
        "with open(output_train, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(train_lines_full)\n",
        "\n",
        "with open(output_val, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(val_lines_full)\n",
        "\n",
        "print(\"train_fullpath.txt ve val_fullpath.txt oluşturuldu.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bB1O9_3R1D7",
        "outputId": "f569e852-5888-42ba-9615-cea935c21b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_fullpath.txt ve val_fullpath.txt oluşturuldu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/^if hparams_string:$/if hparams_string:\\n    hparams.parse(hparams_string)/' /content/tacotron2/hparams.py\n"
      ],
      "metadata": {
        "id": "Wl6xKlDQoz2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -o outdir -l logdir --hparams=\"training_files=/content/drive/MyDrive/Tacotron2_Sounds/train_fullpath.txt,validation_files=/content/drive/MyDrive/Tacotron2_Sounds/val_fullpath.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH053Gt4qFEf",
        "outputId": "e8f54593-80f9-475d-cab7-663b5451e741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 Run: False\n",
            "Dynamic Loss Scaling: True\n",
            "Distributed Run: False\n",
            "cuDNN Enabled: True\n",
            "cuDNN Benchmark: False\n",
            "Epoch: 0\n",
            "/content/tacotron2/utils.py:8: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
            "Train loss 0 61.096096 Grad Norm 13.966157 2.74s/it\n",
            "Validation loss 0: 61.257839  \n",
            "Saving model and optimizer state at iteration 0 to outdir/checkpoint_0\n",
            "Epoch: 1\n",
            "Train loss 1 52.201317 Grad Norm 28.473038 1.42s/it\n",
            "Epoch: 2\n",
            "Train loss 2 28.855556 Grad Norm 25.042871 1.40s/it\n",
            "Epoch: 3\n",
            "Train loss 3 16.704088 Grad Norm 21.240051 1.59s/it\n",
            "Epoch: 4\n",
            "Train loss 4 10.527084 Grad Norm 12.683987 1.38s/it\n",
            "Epoch: 5\n",
            "Train loss 5 9.495444 Grad Norm 12.359615 1.41s/it\n",
            "Epoch: 6\n",
            "Train loss 6 9.193680 Grad Norm 16.567120 1.40s/it\n",
            "Epoch: 7\n",
            "Train loss 7 7.561778 Grad Norm 24.582384 1.63s/it\n",
            "Epoch: 8\n",
            "Train loss 8 8.039953 Grad Norm 12.083736 1.54s/it\n",
            "Epoch: 9\n",
            "Train loss 9 6.507946 Grad Norm 7.884476 1.63s/it\n",
            "Epoch: 10\n",
            "Train loss 10 6.666049 Grad Norm 7.227474 1.41s/it\n",
            "Epoch: 11\n",
            "Train loss 11 5.600828 Grad Norm 18.001841 1.66s/it\n",
            "Epoch: 12\n",
            "Train loss 12 6.054130 Grad Norm 10.765048 1.39s/it\n",
            "Epoch: 13\n",
            "Train loss 13 5.118474 Grad Norm 6.479681 1.84s/it\n",
            "Epoch: 14\n",
            "Train loss 14 5.693267 Grad Norm 15.379194 1.45s/it\n",
            "Epoch: 15\n",
            "Train loss 15 4.682033 Grad Norm 9.146163 1.65s/it\n",
            "Epoch: 16\n",
            "Train loss 16 5.185348 Grad Norm 10.487710 1.45s/it\n",
            "Epoch: 17\n",
            "Train loss 17 4.317282 Grad Norm 7.003477 1.65s/it\n",
            "Epoch: 18\n",
            "Train loss 18 4.788916 Grad Norm 6.882529 1.56s/it\n",
            "Epoch: 19\n",
            "Train loss 19 3.691001 Grad Norm 4.795012 1.64s/it\n",
            "Epoch: 20\n",
            "Train loss 20 4.277955 Grad Norm 7.259780 1.44s/it\n",
            "Epoch: 21\n",
            "Train loss 21 4.092439 Grad Norm 6.255731 1.42s/it\n",
            "Epoch: 22\n",
            "Train loss 22 3.314930 Grad Norm 2.932033 1.68s/it\n",
            "Epoch: 23\n",
            "Train loss 23 3.147111 Grad Norm 3.579092 1.81s/it\n",
            "Epoch: 24\n",
            "Train loss 24 3.596405 Grad Norm 7.291866 1.41s/it\n",
            "Epoch: 25\n",
            "Train loss 25 3.578897 Grad Norm 12.345858 1.43s/it\n",
            "Epoch: 26\n",
            "Train loss 26 3.655141 Grad Norm 8.883200 1.43s/it\n",
            "Epoch: 27\n",
            "Train loss 27 2.828291 Grad Norm 5.781187 1.68s/it\n",
            "Epoch: 28\n",
            "Train loss 28 2.745022 Grad Norm 4.769082 1.81s/it\n",
            "Epoch: 29\n",
            "Train loss 29 3.323502 Grad Norm 13.689409 1.45s/it\n",
            "Epoch: 30\n",
            "Train loss 30 2.705755 Grad Norm 6.881007 1.66s/it\n",
            "Epoch: 31\n",
            "Train loss 31 2.628691 Grad Norm 8.288054 1.70s/it\n",
            "Epoch: 32\n",
            "Train loss 32 2.514872 Grad Norm 4.336094 1.68s/it\n",
            "Epoch: 33\n",
            "Train loss 33 2.633014 Grad Norm 13.199243 1.93s/it\n",
            "Epoch: 34\n",
            "Train loss 34 2.581629 Grad Norm 9.629306 1.69s/it\n",
            "Epoch: 35\n",
            "Train loss 35 3.189366 Grad Norm 10.223533 1.46s/it\n",
            "Epoch: 36\n",
            "Train loss 36 3.046177 Grad Norm 8.630550 1.41s/it\n",
            "Epoch: 37\n",
            "Train loss 37 2.746751 Grad Norm 3.079085 1.64s/it\n",
            "Epoch: 38\n",
            "Train loss 38 2.908895 Grad Norm 6.825141 1.47s/it\n",
            "Epoch: 39\n",
            "Train loss 39 2.929408 Grad Norm 7.464700 1.47s/it\n",
            "Epoch: 40\n",
            "Train loss 40 2.854417 Grad Norm 3.458031 1.43s/it\n",
            "Epoch: 41\n",
            "Train loss 41 2.566055 Grad Norm 3.427266 1.72s/it\n",
            "Epoch: 42\n",
            "Train loss 42 2.462600 Grad Norm 7.778448 1.91s/it\n",
            "Epoch: 43\n",
            "Train loss 43 2.877930 Grad Norm 9.169991 1.44s/it\n",
            "Epoch: 44\n",
            "Train loss 44 2.782462 Grad Norm 15.689962 1.71s/it\n",
            "Epoch: 45\n",
            "Train loss 45 2.616366 Grad Norm 13.797688 1.71s/it\n",
            "Epoch: 46\n",
            "Train loss 46 3.140674 Grad Norm 9.929827 1.46s/it\n",
            "Epoch: 47\n",
            "Train loss 47 3.107316 Grad Norm 9.512837 1.64s/it\n",
            "Epoch: 48\n",
            "Train loss 48 2.442018 Grad Norm 8.259243 1.71s/it\n",
            "Epoch: 49\n",
            "Train loss 49 2.790940 Grad Norm 6.040574 1.49s/it\n",
            "Epoch: 50\n",
            "Train loss 50 2.254158 Grad Norm 2.105804 1.71s/it\n",
            "Epoch: 51\n",
            "Train loss 51 2.685306 Grad Norm 8.227160 1.48s/it\n",
            "Epoch: 52\n",
            "Train loss 52 2.382227 Grad Norm 8.499179 1.85s/it\n",
            "Epoch: 53\n",
            "Train loss 53 2.297214 Grad Norm 2.887880 1.72s/it\n",
            "Epoch: 54\n",
            "Train loss 54 2.289509 Grad Norm 6.974890 1.72s/it\n",
            "Epoch: 55\n",
            "Train loss 55 2.216848 Grad Norm 2.408686 1.73s/it\n",
            "Epoch: 56\n",
            "Train loss 56 2.349216 Grad Norm 12.196766 1.74s/it\n",
            "Epoch: 57\n",
            "Train loss 57 2.736384 Grad Norm 4.653707 1.65s/it\n",
            "Epoch: 58\n",
            "Train loss 58 2.943384 Grad Norm 14.430125 1.70s/it\n",
            "Epoch: 59\n",
            "Train loss 59 2.229077 Grad Norm 6.551636 1.75s/it\n",
            "Epoch: 60\n",
            "Train loss 60 2.248528 Grad Norm 9.692767 1.74s/it\n",
            "Epoch: 61\n",
            "Train loss 61 2.261801 Grad Norm 10.873756 1.93s/it\n",
            "Epoch: 62\n",
            "Train loss 62 2.146928 Grad Norm 2.337486 1.73s/it\n",
            "Epoch: 63\n",
            "Train loss 63 2.220283 Grad Norm 9.251925 1.75s/it\n",
            "Epoch: 64\n",
            "Train loss 64 2.194288 Grad Norm 6.421456 1.74s/it\n",
            "Epoch: 65\n",
            "Train loss 65 2.262707 Grad Norm 7.774191 1.76s/it\n",
            "Epoch: 66\n",
            "Train loss 66 2.156062 Grad Norm 4.207302 1.95s/it\n",
            "Epoch: 67\n",
            "Train loss 67 3.060274 Grad Norm 17.412609 1.47s/it\n",
            "Epoch: 68\n",
            "Train loss 68 3.090235 Grad Norm 17.750656 1.46s/it\n",
            "Epoch: 69\n",
            "Train loss 69 2.457383 Grad Norm 4.693325 1.49s/it\n",
            "Epoch: 70\n",
            "Train loss 70 2.736969 Grad Norm 22.430164 1.73s/it\n",
            "Epoch: 71\n",
            "Train loss 71 3.199327 Grad Norm 28.111904 1.93s/it\n",
            "Epoch: 72\n",
            "Train loss 72 3.115211 Grad Norm 20.049274 1.51s/it\n",
            "Epoch: 73\n",
            "Train loss 73 2.407323 Grad Norm 13.037909 1.71s/it\n",
            "Epoch: 74\n",
            "Train loss 74 2.302337 Grad Norm 5.915862 1.75s/it\n",
            "Epoch: 75\n",
            "Train loss 75 2.320693 Grad Norm 8.523233 1.90s/it\n",
            "Epoch: 76\n",
            "Train loss 76 2.705764 Grad Norm 10.114819 1.44s/it\n",
            "Epoch: 77\n",
            "Train loss 77 2.227559 Grad Norm 7.112257 1.72s/it\n",
            "Epoch: 78\n",
            "Train loss 78 2.215431 Grad Norm 7.284997 1.72s/it\n",
            "Epoch: 79\n",
            "Train loss 79 2.058129 Grad Norm 1.817166 1.72s/it\n",
            "Epoch: 80\n",
            "Train loss 80 2.486027 Grad Norm 7.007937 1.59s/it\n",
            "Epoch: 81\n",
            "Train loss 81 2.023305 Grad Norm 3.014657 1.72s/it\n",
            "Epoch: 82\n",
            "Train loss 82 2.370450 Grad Norm 3.361249 1.47s/it\n",
            "Epoch: 83\n",
            "Train loss 83 2.031738 Grad Norm 3.138688 1.72s/it\n",
            "Epoch: 84\n",
            "Train loss 84 2.050341 Grad Norm 2.573684 1.73s/it\n",
            "Epoch: 85\n",
            "Train loss 85 2.312569 Grad Norm 1.661742 1.64s/it\n",
            "Epoch: 86\n",
            "Train loss 86 2.230262 Grad Norm 11.086334 1.73s/it\n",
            "Epoch: 87\n",
            "Train loss 87 2.431636 Grad Norm 6.504096 1.43s/it\n",
            "Epoch: 88\n",
            "Train loss 88 2.154303 Grad Norm 4.491893 1.73s/it\n",
            "Epoch: 89\n",
            "Train loss 89 2.429322 Grad Norm 8.247987 1.59s/it\n",
            "Epoch: 90\n",
            "Train loss 90 2.109865 Grad Norm 4.426554 1.81s/it\n",
            "Epoch: 91\n",
            "Train loss 91 2.291690 Grad Norm 2.217388 1.49s/it\n",
            "Epoch: 92\n",
            "Train loss 92 2.335692 Grad Norm 4.040723 1.46s/it\n",
            "Epoch: 93\n",
            "Train loss 93 2.084159 Grad Norm 7.908177 1.74s/it\n",
            "Epoch: 94\n",
            "Train loss 94 2.075961 Grad Norm 7.810515 1.89s/it\n",
            "Epoch: 95\n",
            "Train loss 95 2.056496 Grad Norm 3.805628 1.73s/it\n",
            "Epoch: 96\n",
            "Train loss 96 2.302638 Grad Norm 2.330864 1.50s/it\n",
            "Epoch: 97\n",
            "Train loss 97 2.273128 Grad Norm 3.013166 1.49s/it\n",
            "Epoch: 98\n",
            "Train loss 98 2.013770 Grad Norm 3.522937 1.75s/it\n",
            "Epoch: 99\n",
            "Train loss 99 1.951114 Grad Norm 3.410470 1.87s/it\n",
            "Epoch: 100\n",
            "Train loss 100 2.344071 Grad Norm 3.194429 1.48s/it\n",
            "Epoch: 101\n",
            "Train loss 101 1.957415 Grad Norm 2.315771 1.73s/it\n",
            "Epoch: 102\n",
            "Train loss 102 2.413850 Grad Norm 7.535575 1.52s/it\n",
            "Epoch: 103\n",
            "Train loss 103 1.893559 Grad Norm 2.321624 1.72s/it\n",
            "Epoch: 104\n",
            "Train loss 104 1.905143 Grad Norm 3.707232 1.89s/it\n",
            "Epoch: 105\n",
            "Train loss 105 2.303098 Grad Norm 6.358337 1.49s/it\n",
            "Epoch: 106\n",
            "Train loss 106 1.958074 Grad Norm 2.581711 1.73s/it\n",
            "Epoch: 107\n",
            "Train loss 107 1.964123 Grad Norm 4.941524 1.97s/it\n",
            "Epoch: 108\n",
            "Train loss 108 1.898234 Grad Norm 2.326671 1.92s/it\n",
            "Epoch: 109\n",
            "Train loss 109 2.447841 Grad Norm 9.772141 1.49s/it\n",
            "Epoch: 110\n",
            "Train loss 110 1.930500 Grad Norm 3.936162 1.72s/it\n",
            "Epoch: 111\n",
            "Train loss 111 2.229851 Grad Norm 3.381778 1.50s/it\n",
            "Epoch: 112\n",
            "Train loss 112 1.926048 Grad Norm 5.405233 1.73s/it\n",
            "Epoch: 113\n",
            "Train loss 113 1.886007 Grad Norm 2.413706 1.93s/it\n",
            "Epoch: 114\n",
            "Train loss 114 1.809366 Grad Norm 2.274668 1.72s/it\n",
            "Epoch: 115\n",
            "Train loss 115 2.198013 Grad Norm 3.601502 1.46s/it\n",
            "Epoch: 116\n",
            "Train loss 116 2.193490 Grad Norm 4.591280 1.47s/it\n",
            "Epoch: 117\n",
            "Train loss 117 1.850455 Grad Norm 0.990197 1.72s/it\n",
            "Epoch: 118\n",
            "Train loss 118 1.850595 Grad Norm 2.956293 1.84s/it\n",
            "Epoch: 119\n",
            "Train loss 119 1.936160 Grad Norm 4.401721 1.72s/it\n",
            "Epoch: 120\n",
            "Train loss 120 1.838700 Grad Norm 2.776447 1.73s/it\n",
            "Epoch: 121\n",
            "Train loss 121 2.209976 Grad Norm 6.099227 1.48s/it\n",
            "Epoch: 122\n",
            "Train loss 122 1.828610 Grad Norm 1.803288 1.74s/it\n",
            "Epoch: 123\n",
            "Train loss 123 1.868686 Grad Norm 4.434823 1.97s/it\n",
            "Epoch: 124\n",
            "Train loss 124 1.893149 Grad Norm 1.612827 1.74s/it\n",
            "Epoch: 125\n",
            "Train loss 125 2.117658 Grad Norm 5.818519 1.49s/it\n",
            "Epoch: 126\n",
            "Train loss 126 1.848814 Grad Norm 2.596902 1.75s/it\n",
            "Epoch: 127\n",
            "Train loss 127 2.061142 Grad Norm 1.486075 1.63s/it\n",
            "Epoch: 128\n",
            "Train loss 128 1.863222 Grad Norm 3.392904 1.70s/it\n",
            "Epoch: 129\n",
            "Train loss 129 1.931631 Grad Norm 3.113648 1.75s/it\n",
            "Epoch: 130\n",
            "Train loss 130 1.838734 Grad Norm 1.926035 1.74s/it\n",
            "Epoch: 131\n",
            "Train loss 131 1.926899 Grad Norm 4.732660 1.71s/it\n",
            "Epoch: 132\n",
            "Train loss 132 2.183487 Grad Norm 4.225400 1.57s/it\n",
            "Epoch: 133\n",
            "Train loss 133 2.160164 Grad Norm 3.512030 1.49s/it\n",
            "Epoch: 134\n",
            "Train loss 134 2.130081 Grad Norm 2.553509 1.50s/it\n",
            "Epoch: 135\n",
            "Train loss 135 2.064635 Grad Norm 2.723702 1.47s/it\n",
            "Epoch: 136\n",
            "Train loss 136 1.808360 Grad Norm 2.648243 1.74s/it\n",
            "Epoch: 137\n",
            "Train loss 137 2.077664 Grad Norm 3.902136 1.61s/it\n",
            "Epoch: 138\n",
            "Train loss 138 2.116396 Grad Norm 4.190495 1.48s/it\n",
            "Epoch: 139\n",
            "Train loss 139 2.052162 Grad Norm 2.154625 1.48s/it\n",
            "Epoch: 140\n",
            "Train loss 140 1.717962 Grad Norm 1.987268 1.73s/it\n",
            "Epoch: 141\n",
            "Train loss 141 2.035021 Grad Norm 2.804721 1.49s/it\n",
            "Epoch: 142\n",
            "Train loss 142 1.734600 Grad Norm 2.629912 1.85s/it\n",
            "Epoch: 143\n",
            "Train loss 143 1.730893 Grad Norm 3.511158 1.73s/it\n",
            "Epoch: 144\n",
            "Train loss 144 2.056633 Grad Norm 3.599901 1.49s/it\n",
            "Epoch: 145\n",
            "Train loss 145 2.134531 Grad Norm 6.968264 1.45s/it\n",
            "Epoch: 146\n",
            "Train loss 146 1.786013 Grad Norm 5.297224 1.74s/it\n",
            "Epoch: 147\n",
            "Train loss 147 1.766232 Grad Norm 3.414980 1.96s/it\n",
            "Epoch: 148\n",
            "Train loss 148 1.740313 Grad Norm 3.445533 1.74s/it\n",
            "Epoch: 149\n",
            "Train loss 149 1.950705 Grad Norm 1.532887 1.48s/it\n",
            "Epoch: 150\n",
            "Train loss 150 1.702766 Grad Norm 4.817062 1.73s/it\n",
            "Epoch: 151\n",
            "Train loss 151 1.943202 Grad Norm 1.785427 1.60s/it\n",
            "Epoch: 152\n",
            "Train loss 152 1.901016 Grad Norm 1.719433 1.47s/it\n",
            "Epoch: 153\n",
            "Train loss 153 1.797111 Grad Norm 6.315702 1.73s/it\n",
            "Epoch: 154\n",
            "Train loss 154 1.935265 Grad Norm 2.934821 1.48s/it\n",
            "Epoch: 155\n",
            "Train loss 155 1.716137 Grad Norm 3.978208 1.74s/it\n",
            "Epoch: 156\n",
            "Train loss 156 1.665473 Grad Norm 2.873265 1.91s/it\n",
            "Epoch: 157\n",
            "Train loss 157 1.914922 Grad Norm 3.667441 1.49s/it\n",
            "Epoch: 158\n",
            "Train loss 158 1.693186 Grad Norm 3.747060 1.73s/it\n",
            "Epoch: 159\n",
            "Train loss 159 1.876374 Grad Norm 3.616567 1.49s/it\n",
            "Epoch: 160\n",
            "Train loss 160 1.587009 Grad Norm 1.151646 1.75s/it\n",
            "Epoch: 161\n",
            "Train loss 161 1.633290 Grad Norm 3.772058 1.88s/it\n",
            "Epoch: 162\n",
            "Train loss 162 1.616909 Grad Norm 2.348666 1.73s/it\n",
            "Epoch: 163\n",
            "Train loss 163 1.633458 Grad Norm 4.238992 1.72s/it\n",
            "Epoch: 164\n",
            "Train loss 164 1.646856 Grad Norm 4.067155 1.97s/it\n",
            "Epoch: 165\n",
            "Train loss 165 1.854963 Grad Norm 2.191679 1.64s/it\n",
            "Epoch: 166\n",
            "Train loss 166 1.825818 Grad Norm 3.552192 1.44s/it\n",
            "Epoch: 167\n",
            "Train loss 167 1.629830 Grad Norm 2.441581 1.73s/it\n",
            "Epoch: 168\n",
            "Train loss 168 1.585485 Grad Norm 2.426445 1.74s/it\n",
            "Epoch: 169\n",
            "Train loss 169 1.628463 Grad Norm 2.680325 1.74s/it\n",
            "Epoch: 170\n",
            "Train loss 170 1.560617 Grad Norm 1.602775 1.96s/it\n",
            "Epoch: 171\n",
            "Train loss 171 1.629908 Grad Norm 3.966216 1.73s/it\n",
            "Epoch: 172\n",
            "Train loss 172 1.953961 Grad Norm 4.501253 1.50s/it\n",
            "Epoch: 173\n",
            "Train loss 173 1.962679 Grad Norm 8.310477 1.49s/it\n",
            "Epoch: 174\n",
            "Train loss 174 1.949318 Grad Norm 7.617300 2.43s/it\n",
            "Epoch: 175\n",
            "Train loss 175 1.842580 Grad Norm 1.899862 1.45s/it\n",
            "Epoch: 176\n",
            "Train loss 176 1.935959 Grad Norm 5.784310 1.49s/it\n",
            "Epoch: 177\n",
            "Train loss 177 1.955778 Grad Norm 5.560152 1.47s/it\n",
            "Epoch: 178\n",
            "Train loss 178 1.636100 Grad Norm 3.602787 1.75s/it\n",
            "Epoch: 179\n",
            "Train loss 179 1.892781 Grad Norm 4.306558 1.60s/it\n",
            "Epoch: 180\n",
            "Train loss 180 1.765147 Grad Norm 1.398164 1.47s/it\n",
            "Epoch: 181\n",
            "Train loss 181 1.905127 Grad Norm 6.158288 1.48s/it\n",
            "Epoch: 182\n",
            "Train loss 182 1.981453 Grad Norm 7.051146 1.48s/it\n",
            "Epoch: 183\n",
            "Train loss 183 1.818920 Grad Norm 2.040762 1.49s/it\n",
            "Epoch: 184\n",
            "Train loss 184 1.705049 Grad Norm 6.901284 1.92s/it\n",
            "Epoch: 185\n",
            "Train loss 185 1.614799 Grad Norm 5.436421 1.86s/it\n",
            "Epoch: 186\n",
            "Train loss 186 1.552598 Grad Norm 2.260909 1.73s/it\n",
            "Epoch: 187\n",
            "Train loss 187 1.597650 Grad Norm 3.682734 1.73s/it\n",
            "Epoch: 188\n",
            "Train loss 188 1.573613 Grad Norm 2.212767 1.73s/it\n",
            "Epoch: 189\n",
            "Train loss 189 1.556127 Grad Norm 4.030652 1.97s/it\n",
            "Epoch: 190\n",
            "Train loss 190 1.919904 Grad Norm 3.782196 1.49s/it\n",
            "Epoch: 191\n",
            "Train loss 191 1.842571 Grad Norm 3.123744 1.45s/it\n",
            "Epoch: 192\n",
            "Train loss 192 1.829482 Grad Norm 2.163954 1.47s/it\n",
            "Epoch: 193\n",
            "Train loss 193 1.578472 Grad Norm 4.005751 1.74s/it\n",
            "Epoch: 194\n",
            "Train loss 194 1.561398 Grad Norm 2.798135 1.87s/it\n",
            "Epoch: 195\n",
            "Train loss 195 1.867299 Grad Norm 5.067138 1.49s/it\n",
            "Epoch: 196\n",
            "Train loss 196 1.770731 Grad Norm 3.434464 1.49s/it\n",
            "Epoch: 197\n",
            "Train loss 197 1.587358 Grad Norm 4.797764 1.72s/it\n",
            "Epoch: 198\n",
            "Train loss 198 1.850760 Grad Norm 5.586688 2.12s/it\n",
            "Epoch: 199\n",
            "Train loss 199 1.748581 Grad Norm 1.956513 1.48s/it\n",
            "Epoch: 200\n",
            "Train loss 200 1.729030 Grad Norm 1.758973 1.47s/it\n",
            "Epoch: 201\n",
            "Train loss 201 1.514884 Grad Norm 3.564777 1.97s/it\n",
            "Epoch: 202\n",
            "Train loss 202 1.719778 Grad Norm 2.074839 1.52s/it\n",
            "Epoch: 203\n",
            "Train loss 203 1.551813 Grad Norm 3.728226 1.98s/it\n",
            "Epoch: 204\n",
            "Train loss 204 1.542665 Grad Norm 2.955242 1.74s/it\n",
            "Epoch: 205\n",
            "Train loss 205 1.489041 Grad Norm 3.306736 1.72s/it\n",
            "Epoch: 206\n",
            "Train loss 206 1.484167 Grad Norm 3.088233 1.73s/it\n",
            "Epoch: 207\n",
            "Train loss 207 1.778831 Grad Norm 3.859388 1.62s/it\n",
            "Epoch: 208\n",
            "Train loss 208 1.432772 Grad Norm 1.156251 1.71s/it\n",
            "Epoch: 209\n",
            "Train loss 209 1.489244 Grad Norm 2.710403 1.74s/it\n",
            "Epoch: 210\n",
            "Train loss 210 1.723435 Grad Norm 2.146377 1.51s/it\n",
            "Epoch: 211\n",
            "Train loss 211 1.432020 Grad Norm 1.141482 1.74s/it\n",
            "Epoch: 212\n",
            "Train loss 212 1.455591 Grad Norm 1.387121 1.86s/it\n",
            "Epoch: 213\n",
            "Train loss 213 1.697389 Grad Norm 2.687609 1.49s/it\n",
            "Epoch: 214\n",
            "Train loss 214 1.466172 Grad Norm 3.157034 1.75s/it\n",
            "Epoch: 215\n",
            "Train loss 215 1.418524 Grad Norm 1.684909 1.95s/it\n",
            "Epoch: 216\n",
            "Train loss 216 1.425429 Grad Norm 2.377458 1.77s/it\n",
            "Epoch: 217\n",
            "Train loss 217 1.399953 Grad Norm 1.514598 1.90s/it\n",
            "Epoch: 218\n",
            "Train loss 218 1.669327 Grad Norm 1.633481 1.49s/it\n",
            "Epoch: 219\n",
            "Train loss 219 1.389693 Grad Norm 1.540287 1.72s/it\n",
            "Epoch: 220\n",
            "Train loss 220 1.588980 Grad Norm 1.477549 1.50s/it\n",
            "Epoch: 221\n",
            "Train loss 221 1.401238 Grad Norm 1.944321 1.85s/it\n",
            "Epoch: 222\n",
            "Train loss 222 1.477846 Grad Norm 3.505100 1.72s/it\n",
            "Epoch: 223\n",
            "Train loss 223 1.448112 Grad Norm 1.981747 1.74s/it\n",
            "Epoch: 224\n",
            "Train loss 224 1.646198 Grad Norm 3.137264 1.49s/it\n",
            "Epoch: 225\n",
            "Train loss 225 1.424073 Grad Norm 3.270473 1.75s/it\n",
            "Epoch: 226\n",
            "Train loss 226 1.421872 Grad Norm 2.635504 1.87s/it\n",
            "Epoch: 227\n",
            "Train loss 227 1.626372 Grad Norm 2.999684 1.49s/it\n",
            "Epoch: 228\n",
            "Train loss 228 1.648563 Grad Norm 5.435879 1.47s/it\n",
            "Epoch: 229\n",
            "Train loss 229 1.684793 Grad Norm 5.207202 1.50s/it\n",
            "Epoch: 230\n",
            "Train loss 230 1.362289 Grad Norm 1.251019 1.74s/it\n",
            "Epoch: 231\n",
            "Train loss 231 1.531890 Grad Norm 5.695571 1.89s/it\n",
            "Epoch: 232\n",
            "Train loss 232 1.580227 Grad Norm 6.152681 1.74s/it\n",
            "Epoch: 233\n",
            "Train loss 233 1.694293 Grad Norm 3.475295 1.47s/it\n",
            "Epoch: 234\n",
            "Train loss 234 1.563628 Grad Norm 7.246592 1.74s/it\n",
            "Epoch: 235\n",
            "Train loss 235 1.633721 Grad Norm 8.858397 1.89s/it\n",
            "Epoch: 236\n",
            "Train loss 236 1.485488 Grad Norm 5.067894 1.72s/it\n",
            "Epoch: 237\n",
            "Train loss 237 1.754772 Grad Norm 3.864292 1.49s/it\n",
            "Epoch: 238\n",
            "Train loss 238 1.472491 Grad Norm 4.290493 1.75s/it\n",
            "Epoch: 239\n",
            "Train loss 239 1.379311 Grad Norm 2.511687 1.76s/it\n",
            "Epoch: 240\n",
            "Train loss 240 1.610162 Grad Norm 2.871398 1.67s/it\n",
            "Epoch: 241\n",
            "Train loss 241 1.646158 Grad Norm 4.120097 1.48s/it\n",
            "Epoch: 242\n",
            "Train loss 242 1.571382 Grad Norm 2.078815 1.50s/it\n",
            "Epoch: 243\n",
            "Train loss 243 1.359986 Grad Norm 2.521978 1.74s/it\n",
            "Epoch: 244\n",
            "Train loss 244 1.387105 Grad Norm 2.138708 1.74s/it\n",
            "Epoch: 245\n",
            "Train loss 245 1.572090 Grad Norm 1.590683 1.65s/it\n",
            "Epoch: 246\n",
            "Train loss 246 1.557099 Grad Norm 2.417572 1.50s/it\n",
            "Epoch: 247\n",
            "Train loss 247 1.556298 Grad Norm 2.059447 1.48s/it\n",
            "Epoch: 248\n",
            "Train loss 248 1.557601 Grad Norm 1.569593 1.47s/it\n",
            "Epoch: 249\n",
            "Train loss 249 1.543368 Grad Norm 2.974839 1.49s/it\n",
            "Epoch: 250\n",
            "Train loss 250 1.408065 Grad Norm 2.312859 1.88s/it\n",
            "Epoch: 251\n",
            "Train loss 251 1.604800 Grad Norm 5.003421 1.49s/it\n",
            "Epoch: 252\n",
            "Train loss 252 1.327226 Grad Norm 3.396942 1.74s/it\n",
            "Epoch: 253\n",
            "Train loss 253 1.501209 Grad Norm 1.352689 1.50s/it\n",
            "Epoch: 254\n",
            "Train loss 254 1.329505 Grad Norm 3.317417 1.72s/it\n",
            "Epoch: 255\n",
            "Train loss 255 1.374768 Grad Norm 1.769113 1.94s/it\n",
            "Epoch: 256\n",
            "Train loss 256 1.609071 Grad Norm 3.985102 1.50s/it\n",
            "Epoch: 257\n",
            "Train loss 257 1.368595 Grad Norm 1.998143 1.73s/it\n",
            "Epoch: 258\n",
            "Train loss 258 1.311817 Grad Norm 2.298574 1.75s/it\n",
            "Epoch: 259\n",
            "Train loss 259 1.531720 Grad Norm 1.984351 1.64s/it\n",
            "Epoch: 260\n",
            "Train loss 260 1.286674 Grad Norm 1.302343 1.90s/it\n",
            "Epoch: 261\n",
            "Train loss 261 1.483246 Grad Norm 1.380738 1.49s/it\n",
            "Epoch: 262\n",
            "Train loss 262 1.493906 Grad Norm 1.805132 1.48s/it\n",
            "Epoch: 263\n",
            "Train loss 263 1.446302 Grad Norm 1.683738 1.50s/it\n",
            "Epoch: 264\n",
            "Train loss 264 1.499413 Grad Norm 1.953967 1.63s/it\n",
            "Epoch: 265\n",
            "Train loss 265 1.448231 Grad Norm 0.934420 1.47s/it\n",
            "Epoch: 266\n",
            "Train loss 266 1.458390 Grad Norm 1.521895 1.49s/it\n",
            "Epoch: 267\n",
            "Train loss 267 1.511335 Grad Norm 1.103181 1.42s/it\n",
            "Epoch: 268\n",
            "Train loss 268 1.291639 Grad Norm 2.795712 1.76s/it\n",
            "Epoch: 269\n",
            "Train loss 269 1.285335 Grad Norm 1.894802 1.91s/it\n",
            "Epoch: 270\n",
            "Train loss 270 1.246822 Grad Norm 1.359673 1.93s/it\n",
            "Epoch: 271\n",
            "Train loss 271 1.273197 Grad Norm 2.912532 1.75s/it\n",
            "Epoch: 272\n",
            "Train loss 272 1.461800 Grad Norm 1.746610 1.50s/it\n",
            "Epoch: 273\n",
            "Train loss 273 1.260600 Grad Norm 2.713340 1.73s/it\n",
            "Epoch: 274\n",
            "Train loss 274 1.487260 Grad Norm 2.917447 1.63s/it\n",
            "Epoch: 275\n",
            "Train loss 275 1.275787 Grad Norm 3.751044 1.73s/it\n",
            "Epoch: 276\n",
            "Train loss 276 1.283363 Grad Norm 3.080133 1.74s/it\n",
            "Epoch: 277\n",
            "Train loss 277 1.267598 Grad Norm 1.904970 1.74s/it\n",
            "Epoch: 278\n",
            "Train loss 278 1.221904 Grad Norm 1.658644 1.86s/it\n",
            "Epoch: 279\n",
            "Train loss 279 1.421838 Grad Norm 3.107592 1.52s/it\n",
            "Epoch: 280\n",
            "Train loss 280 1.454708 Grad Norm 2.323649 1.49s/it\n",
            "Epoch: 281\n",
            "Train loss 281 1.482885 Grad Norm 3.800331 1.46s/it\n",
            "Epoch: 282\n",
            "Train loss 282 1.230651 Grad Norm 2.505150 1.74s/it\n",
            "Epoch: 283\n",
            "Train loss 283 1.431327 Grad Norm 3.004591 1.58s/it\n",
            "Epoch: 284\n",
            "Train loss 284 1.250691 Grad Norm 3.231051 1.71s/it\n",
            "Epoch: 285\n",
            "Train loss 285 1.434372 Grad Norm 2.592685 1.48s/it\n",
            "Epoch: 286\n",
            "Train loss 286 1.181213 Grad Norm 1.022237 1.71s/it\n",
            "Epoch: 287\n",
            "Train loss 287 1.406517 Grad Norm 2.632489 1.50s/it\n",
            "Epoch: 288\n",
            "Train loss 288 1.422079 Grad Norm 1.332153 1.67s/it\n",
            "Epoch: 289\n",
            "Train loss 289 1.399140 Grad Norm 1.739132 1.49s/it\n",
            "Epoch: 290\n",
            "Train loss 290 1.216154 Grad Norm 2.027289 1.74s/it\n",
            "Epoch: 291\n",
            "Train loss 291 1.371398 Grad Norm 1.445298 1.48s/it\n",
            "Epoch: 292\n",
            "Train loss 292 1.194687 Grad Norm 1.347528 1.74s/it\n",
            "Epoch: 293\n",
            "Train loss 293 1.168736 Grad Norm 1.211471 1.86s/it\n",
            "Epoch: 294\n",
            "Train loss 294 1.172662 Grad Norm 1.724231 1.71s/it\n",
            "Epoch: 295\n",
            "Train loss 295 1.188001 Grad Norm 1.481606 1.73s/it\n",
            "Epoch: 296\n",
            "Train loss 296 1.386764 Grad Norm 1.503985 1.47s/it\n",
            "Epoch: 297\n",
            "Train loss 297 1.390581 Grad Norm 2.389797 1.48s/it\n",
            "Epoch: 298\n",
            "Train loss 298 1.164802 Grad Norm 1.092740 1.94s/it\n",
            "Epoch: 299\n",
            "Train loss 299 1.379928 Grad Norm 1.958403 1.50s/it\n",
            "Epoch: 300\n",
            "Train loss 300 1.365303 Grad Norm 2.697826 1.47s/it\n",
            "Epoch: 301\n",
            "Train loss 301 1.166971 Grad Norm 1.341929 1.73s/it\n",
            "Epoch: 302\n",
            "Train loss 302 1.385973 Grad Norm 4.173216 1.59s/it\n",
            "Epoch: 303\n",
            "Train loss 303 1.327141 Grad Norm 2.816580 1.59s/it\n",
            "Epoch: 304\n",
            "Train loss 304 1.246550 Grad Norm 5.076548 1.73s/it\n",
            "Epoch: 305\n",
            "Train loss 305 1.224860 Grad Norm 5.451699 1.73s/it\n",
            "Epoch: 306\n",
            "Train loss 306 1.188641 Grad Norm 1.421775 1.75s/it\n",
            "Epoch: 307\n",
            "Train loss 307 1.403649 Grad Norm 5.262054 1.65s/it\n",
            "Epoch: 308\n",
            "Train loss 308 1.318751 Grad Norm 4.902677 1.71s/it\n",
            "Epoch: 309\n",
            "Train loss 309 1.249755 Grad Norm 2.401400 1.72s/it\n",
            "Epoch: 310\n",
            "Train loss 310 1.368855 Grad Norm 4.102855 1.51s/it\n",
            "Epoch: 311\n",
            "Train loss 311 1.404478 Grad Norm 3.821212 1.47s/it\n",
            "Epoch: 312\n",
            "Train loss 312 1.155141 Grad Norm 2.055101 1.89s/it\n",
            "Epoch: 313\n",
            "Train loss 313 1.183374 Grad Norm 2.708805 1.72s/it\n",
            "Epoch: 314\n",
            "Train loss 314 1.205215 Grad Norm 2.395836 1.73s/it\n",
            "Epoch: 315\n",
            "Train loss 315 1.171590 Grad Norm 2.249904 1.75s/it\n",
            "Epoch: 316\n",
            "Train loss 316 1.152146 Grad Norm 1.534795 1.73s/it\n",
            "Epoch: 317\n",
            "Train loss 317 1.125630 Grad Norm 2.274131 1.93s/it\n",
            "Epoch: 318\n",
            "Train loss 318 1.136317 Grad Norm 1.207046 1.71s/it\n",
            "Epoch: 319\n",
            "Train loss 319 1.117335 Grad Norm 1.314887 1.73s/it\n",
            "Epoch: 320\n",
            "Train loss 320 1.313973 Grad Norm 2.623481 1.48s/it\n",
            "Epoch: 321\n",
            "Train loss 321 1.115227 Grad Norm 1.569201 1.91s/it\n",
            "Epoch: 322\n",
            "Train loss 322 1.107458 Grad Norm 1.207212 1.97s/it\n",
            "Epoch: 323\n",
            "Train loss 323 1.123775 Grad Norm 2.389411 1.75s/it\n",
            "Epoch: 324\n",
            "Train loss 324 1.305786 Grad Norm 3.080841 1.48s/it\n",
            "Epoch: 325\n",
            "Train loss 325 1.317351 Grad Norm 2.699476 1.50s/it\n",
            "Epoch: 326\n",
            "Train loss 326 1.291733 Grad Norm 2.314756 1.66s/it\n",
            "Epoch: 327\n",
            "Train loss 327 1.264092 Grad Norm 2.627413 1.50s/it\n",
            "Epoch: 328\n",
            "Train loss 328 1.102791 Grad Norm 1.279518 1.74s/it\n",
            "Epoch: 329\n",
            "Train loss 329 1.093002 Grad Norm 2.267757 1.72s/it\n",
            "Epoch: 330\n",
            "Train loss 330 1.256361 Grad Norm 1.481035 1.47s/it\n",
            "Epoch: 331\n",
            "Train loss 331 1.268792 Grad Norm 2.874880 1.59s/it\n",
            "Epoch: 332\n",
            "Train loss 332 1.098362 Grad Norm 1.382389 1.75s/it\n",
            "Epoch: 333\n",
            "Train loss 333 1.076751 Grad Norm 1.800965 1.73s/it\n",
            "Epoch: 334\n",
            "Train loss 334 1.062605 Grad Norm 1.346806 1.73s/it\n",
            "Epoch: 335\n",
            "Train loss 335 1.054026 Grad Norm 1.127697 1.75s/it\n",
            "Epoch: 336\n",
            "Train loss 336 1.103498 Grad Norm 3.172212 1.89s/it\n",
            "Epoch: 337\n",
            "Train loss 337 1.100389 Grad Norm 2.594047 1.73s/it\n",
            "Epoch: 338\n",
            "Train loss 338 1.275683 Grad Norm 3.431280 1.49s/it\n",
            "Epoch: 339\n",
            "Train loss 339 1.049631 Grad Norm 1.643375 1.74s/it\n",
            "Epoch: 340\n",
            "Train loss 340 1.266841 Grad Norm 2.946931 1.61s/it\n",
            "Epoch: 341\n",
            "Train loss 341 1.064790 Grad Norm 3.125338 1.71s/it\n",
            "Epoch: 342\n",
            "Train loss 342 1.238743 Grad Norm 2.236789 1.49s/it\n",
            "Epoch: 343\n",
            "Train loss 343 1.243445 Grad Norm 3.079110 1.48s/it\n",
            "Epoch: 344\n",
            "Train loss 344 1.209151 Grad Norm 1.972438 1.48s/it\n",
            "Epoch: 345\n",
            "Train loss 345 1.070257 Grad Norm 1.836275 1.90s/it\n",
            "Epoch: 346\n",
            "Train loss 346 1.078061 Grad Norm 2.415866 1.74s/it\n",
            "Epoch: 347\n",
            "Train loss 347 1.054983 Grad Norm 1.812125 1.74s/it\n",
            "Epoch: 348\n",
            "Train loss 348 1.091283 Grad Norm 3.628789 1.74s/it\n",
            "Epoch: 349\n",
            "Train loss 349 1.222505 Grad Norm 3.507969 1.48s/it\n",
            "Epoch: 350\n",
            "Train loss 350 1.041766 Grad Norm 1.298972 1.90s/it\n",
            "Epoch: 351\n",
            "Train loss 351 1.019004 Grad Norm 0.975590 1.74s/it\n",
            "Epoch: 352\n",
            "Train loss 352 1.209533 Grad Norm 2.622749 1.48s/it\n",
            "Epoch: 353\n",
            "Train loss 353 1.029390 Grad Norm 1.512843 1.75s/it\n",
            "Epoch: 354\n",
            "Train loss 354 1.266743 Grad Norm 5.103316 1.64s/it\n",
            "Epoch: 355\n",
            "Train loss 355 1.290346 Grad Norm 5.234995 1.50s/it\n",
            "Epoch: 356\n",
            "Train loss 356 1.206513 Grad Norm 1.408731 1.49s/it\n",
            "Epoch: 357\n",
            "Train loss 357 1.320338 Grad Norm 6.512358 1.47s/it\n",
            "Epoch: 358\n",
            "Train loss 358 1.255241 Grad Norm 6.968305 1.73s/it\n",
            "Epoch: 359\n",
            "Train loss 359 1.275153 Grad Norm 3.778311 1.60s/it\n",
            "Epoch: 360\n",
            "Train loss 360 1.063488 Grad Norm 2.868407 1.71s/it\n",
            "Epoch: 361\n",
            "Train loss 361 1.263750 Grad Norm 4.725067 1.50s/it\n",
            "Epoch: 362\n",
            "Train loss 362 1.089083 Grad Norm 2.423473 1.72s/it\n",
            "Epoch: 363\n",
            "Train loss 363 1.266744 Grad Norm 2.974544 1.46s/it\n",
            "Epoch: 364\n",
            "Train loss 364 1.232230 Grad Norm 3.018781 1.62s/it\n",
            "Epoch: 365\n",
            "Train loss 365 1.054252 Grad Norm 1.980510 1.73s/it\n",
            "Epoch: 366\n",
            "Train loss 366 1.161447 Grad Norm 2.575938 1.49s/it\n",
            "Epoch: 367\n",
            "Train loss 367 1.025591 Grad Norm 1.863761 1.75s/it\n",
            "Epoch: 368\n",
            "Train loss 368 1.181176 Grad Norm 1.582711 1.51s/it\n",
            "Epoch: 369\n",
            "Train loss 369 1.189103 Grad Norm 1.965793 1.58s/it\n",
            "Epoch: 370\n",
            "Train loss 370 1.037673 Grad Norm 2.000288 1.73s/it\n",
            "Epoch: 371\n",
            "Train loss 371 1.149095 Grad Norm 1.033075 1.49s/it\n",
            "Epoch: 372\n",
            "Train loss 372 1.012874 Grad Norm 1.672078 1.72s/it\n",
            "Epoch: 373\n",
            "Train loss 373 1.073219 Grad Norm 2.693172 1.74s/it\n",
            "Epoch: 374\n",
            "Train loss 374 1.014640 Grad Norm 1.864956 1.94s/it\n",
            "Epoch: 375\n",
            "Train loss 375 1.130857 Grad Norm 1.984083 1.48s/it\n",
            "Epoch: 376\n",
            "Train loss 376 0.994125 Grad Norm 2.458280 1.73s/it\n",
            "Epoch: 377\n",
            "Train loss 377 1.025756 Grad Norm 1.647591 1.95s/it\n",
            "Epoch: 378\n",
            "Train loss 378 1.150209 Grad Norm 1.614312 1.60s/it\n",
            "Epoch: 379\n",
            "Train loss 379 1.179119 Grad Norm 2.931624 1.47s/it\n",
            "Epoch: 380\n",
            "Train loss 380 1.011890 Grad Norm 2.030638 1.76s/it\n",
            "Epoch: 381\n",
            "Train loss 381 1.190507 Grad Norm 4.059263 1.48s/it\n",
            "Epoch: 382\n",
            "Train loss 382 1.182938 Grad Norm 3.926519 1.49s/it\n",
            "Epoch: 383\n",
            "Train loss 383 0.997125 Grad Norm 1.649968 1.90s/it\n",
            "Epoch: 384\n",
            "Train loss 384 0.983156 Grad Norm 2.402630 1.73s/it\n",
            "Epoch: 385\n",
            "Train loss 385 0.956689 Grad Norm 1.072464 1.73s/it\n",
            "Epoch: 386\n",
            "Train loss 386 1.159700 Grad Norm 3.301403 1.48s/it\n",
            "Epoch: 387\n",
            "Train loss 387 1.100729 Grad Norm 1.382408 1.51s/it\n",
            "Epoch: 388\n",
            "Train loss 388 1.013226 Grad Norm 3.820506 1.87s/it\n",
            "Epoch: 389\n",
            "Train loss 389 1.016969 Grad Norm 3.780739 1.74s/it\n",
            "Epoch: 390\n",
            "Train loss 390 1.107660 Grad Norm 1.511722 1.49s/it\n",
            "Epoch: 391\n",
            "Train loss 391 1.080579 Grad Norm 1.938329 1.49s/it\n",
            "Epoch: 392\n",
            "Train loss 392 1.012663 Grad Norm 1.631287 1.76s/it\n",
            "Epoch: 393\n",
            "Train loss 393 1.074978 Grad Norm 0.956723 1.66s/it\n",
            "Epoch: 394\n",
            "Train loss 394 1.118911 Grad Norm 1.767194 1.46s/it\n",
            "Epoch: 395\n",
            "Train loss 395 0.949003 Grad Norm 1.465420 1.73s/it\n",
            "Epoch: 396\n",
            "Train loss 396 0.920327 Grad Norm 0.950402 1.74s/it\n",
            "Epoch: 397\n",
            "Train loss 397 0.962608 Grad Norm 2.280008 1.85s/it\n",
            "Epoch: 398\n",
            "Train loss 398 0.974225 Grad Norm 1.095805 1.74s/it\n",
            "Epoch: 399\n",
            "Train loss 399 0.950668 Grad Norm 2.648878 1.76s/it\n",
            "Epoch: 400\n",
            "Train loss 400 0.963086 Grad Norm 1.459263 1.71s/it\n",
            "Epoch: 401\n",
            "Train loss 401 0.993793 Grad Norm 3.303480 1.74s/it\n",
            "Epoch: 402\n",
            "Train loss 402 0.974966 Grad Norm 2.930559 1.91s/it\n",
            "Epoch: 403\n",
            "Train loss 403 0.947764 Grad Norm 1.584995 1.72s/it\n",
            "Epoch: 404\n",
            "Train loss 404 1.077204 Grad Norm 1.695038 1.50s/it\n",
            "Epoch: 405\n",
            "Train loss 405 0.947826 Grad Norm 1.596001 1.71s/it\n",
            "Epoch: 406\n",
            "Train loss 406 0.943171 Grad Norm 1.294543 1.73s/it\n",
            "Epoch: 407\n",
            "Train loss 407 0.961614 Grad Norm 2.822378 1.91s/it\n",
            "Epoch: 408\n",
            "Train loss 408 1.109543 Grad Norm 2.496342 1.47s/it\n",
            "Epoch: 409\n",
            "Train loss 409 1.054830 Grad Norm 2.879290 1.49s/it\n",
            "Epoch: 410\n",
            "Train loss 410 0.925534 Grad Norm 1.951031 1.74s/it\n",
            "Epoch: 411\n",
            "Train loss 411 1.035197 Grad Norm 1.890335 1.57s/it\n",
            "Epoch: 412\n",
            "Train loss 412 0.918261 Grad Norm 1.367169 1.84s/it\n",
            "Epoch: 413\n",
            "Train loss 413 0.921121 Grad Norm 2.160285 1.75s/it\n",
            "Epoch: 414\n",
            "Train loss 414 0.906630 Grad Norm 1.358189 1.74s/it\n",
            "Epoch: 415\n",
            "Train loss 415 1.087476 Grad Norm 2.701596 1.49s/it\n",
            "Epoch: 416\n",
            "Train loss 416 0.910816 Grad Norm 2.456316 1.91s/it\n",
            "Epoch: 417\n",
            "Train loss 417 0.888308 Grad Norm 1.652302 1.73s/it\n",
            "Epoch: 418\n",
            "Train loss 418 1.053318 Grad Norm 3.111727 1.50s/it\n",
            "Epoch: 419\n",
            "Train loss 419 0.878998 Grad Norm 2.062052 1.74s/it\n",
            "Epoch: 420\n",
            "Train loss 420 0.916482 Grad Norm 1.937622 1.76s/it\n",
            "Epoch: 421\n",
            "Train loss 421 1.081909 Grad Norm 3.390419 1.61s/it\n",
            "Epoch: 422\n",
            "Train loss 422 1.031423 Grad Norm 3.087776 1.49s/it\n",
            "Epoch: 423\n",
            "Train loss 423 0.902841 Grad Norm 1.732272 1.77s/it\n",
            "Epoch: 424\n",
            "Train loss 424 1.010857 Grad Norm 1.573012 1.49s/it\n",
            "Epoch: 425\n",
            "Train loss 425 1.034425 Grad Norm 2.198967 1.49s/it\n",
            "Epoch: 426\n",
            "Train loss 426 0.862423 Grad Norm 0.990189 1.86s/it\n",
            "Epoch: 427\n",
            "Train loss 427 1.050943 Grad Norm 3.412623 1.71s/it\n",
            "Epoch: 428\n",
            "Train loss 428 0.927485 Grad Norm 3.227246 1.72s/it\n",
            "Epoch: 429\n",
            "Train loss 429 1.020645 Grad Norm 1.645997 1.46s/it\n",
            "Epoch: 430\n",
            "Train loss 430 0.896518 Grad Norm 1.594297 1.73s/it\n",
            "Epoch: 431\n",
            "Train loss 431 0.900742 Grad Norm 1.512367 1.93s/it\n",
            "Epoch: 432\n",
            "Train loss 432 0.982863 Grad Norm 0.823493 1.52s/it\n",
            "Epoch: 433\n",
            "Train loss 433 0.914888 Grad Norm 1.751975 1.72s/it\n",
            "Epoch: 434\n",
            "Train loss 434 1.004840 Grad Norm 1.233402 1.50s/it\n",
            "Epoch: 435\n",
            "Train loss 435 0.909063 Grad Norm 3.006681 1.88s/it\n",
            "Epoch: 436\n",
            "Train loss 436 0.897398 Grad Norm 2.188037 1.72s/it\n",
            "Epoch: 437\n",
            "Train loss 437 1.035367 Grad Norm 3.331783 1.50s/it\n",
            "Epoch: 438\n",
            "Train loss 438 1.018606 Grad Norm 2.918929 1.49s/it\n",
            "Epoch: 439\n",
            "Train loss 439 1.007940 Grad Norm 2.054514 1.49s/it\n",
            "Epoch: 440\n",
            "Train loss 440 0.907980 Grad Norm 1.988437 1.90s/it\n",
            "Epoch: 441\n",
            "Train loss 441 0.861218 Grad Norm 1.520219 1.73s/it\n",
            "Epoch: 442\n",
            "Train loss 442 0.991115 Grad Norm 1.895553 1.50s/it\n",
            "Epoch: 443\n",
            "Train loss 443 0.872832 Grad Norm 2.524905 1.73s/it\n",
            "Epoch: 444\n",
            "Train loss 444 0.998354 Grad Norm 1.867636 1.50s/it\n",
            "Epoch: 445\n",
            "Train loss 445 1.042949 Grad Norm 3.285335 1.60s/it\n",
            "Epoch: 446\n",
            "Train loss 446 0.855616 Grad Norm 1.855591 1.73s/it\n",
            "Epoch: 447\n",
            "Train loss 447 0.866815 Grad Norm 2.315021 1.75s/it\n",
            "Epoch: 448\n",
            "Train loss 448 0.989756 Grad Norm 2.274697 1.45s/it\n",
            "Epoch: 449\n",
            "Train loss 449 0.960804 Grad Norm 1.832143 1.51s/it\n",
            "Epoch: 450\n",
            "Train loss 450 0.985461 Grad Norm 1.704132 1.60s/it\n",
            "Epoch: 451\n",
            "Train loss 451 0.841199 Grad Norm 2.029981 1.74s/it\n",
            "Epoch: 452\n",
            "Train loss 452 0.993923 Grad Norm 1.326458 1.49s/it\n",
            "Epoch: 453\n",
            "Train loss 453 0.997917 Grad Norm 2.858727 1.48s/it\n",
            "Epoch: 454\n",
            "Train loss 454 0.987260 Grad Norm 2.077683 1.50s/it\n",
            "Epoch: 455\n",
            "Train loss 455 0.974855 Grad Norm 2.523070 1.61s/it\n",
            "Epoch: 456\n",
            "Train loss 456 0.979296 Grad Norm 2.389363 1.49s/it\n",
            "Epoch: 457\n",
            "Train loss 457 0.856623 Grad Norm 2.057069 1.73s/it\n",
            "Epoch: 458\n",
            "Train loss 458 0.829023 Grad Norm 1.391301 1.74s/it\n",
            "Epoch: 459\n",
            "Train loss 459 0.850186 Grad Norm 2.339694 1.74s/it\n",
            "Epoch: 460\n",
            "Train loss 460 0.844944 Grad Norm 2.224318 1.88s/it\n",
            "Epoch: 461\n",
            "Train loss 461 0.976825 Grad Norm 2.620179 1.47s/it\n",
            "Epoch: 462\n",
            "Train loss 462 0.825296 Grad Norm 1.847261 1.75s/it\n",
            "Epoch: 463\n",
            "Train loss 463 0.969759 Grad Norm 2.053700 1.47s/it\n",
            "Epoch: 464\n",
            "Train loss 464 0.830406 Grad Norm 1.676007 1.87s/it\n",
            "Epoch: 465\n",
            "Train loss 465 0.833306 Grad Norm 2.089064 1.78s/it\n",
            "Epoch: 466\n",
            "Train loss 466 0.803910 Grad Norm 1.562963 1.73s/it\n",
            "Epoch: 467\n",
            "Train loss 467 0.953823 Grad Norm 2.258049 1.51s/it\n",
            "Epoch: 468\n",
            "Train loss 468 0.811258 Grad Norm 2.358220 1.73s/it\n",
            "Epoch: 469\n",
            "Train loss 469 0.817902 Grad Norm 1.456110 1.86s/it\n",
            "Epoch: 470\n",
            "Train loss 470 0.783139 Grad Norm 1.023544 1.72s/it\n",
            "Epoch: 471\n",
            "Train loss 471 0.929285 Grad Norm 2.359680 1.50s/it\n",
            "Epoch: 472\n",
            "Train loss 472 0.796827 Grad Norm 1.876154 1.74s/it\n",
            "Epoch: 473\n",
            "Train loss 473 0.965259 Grad Norm 2.646210 1.50s/it\n",
            "Epoch: 474\n",
            "Train loss 474 0.808357 Grad Norm 1.627319 2.01s/it\n",
            "Epoch: 475\n",
            "Train loss 475 0.941556 Grad Norm 2.186416 1.50s/it\n",
            "Epoch: 476\n",
            "Train loss 476 0.912227 Grad Norm 1.929223 1.50s/it\n",
            "Epoch: 477\n",
            "Train loss 477 0.922657 Grad Norm 2.255679 1.48s/it\n",
            "Epoch: 478\n",
            "Train loss 478 0.951984 Grad Norm 2.145560 1.60s/it\n",
            "Epoch: 479\n",
            "Train loss 479 0.952438 Grad Norm 2.663486 1.62s/it\n",
            "Epoch: 480\n",
            "Train loss 480 0.926049 Grad Norm 2.391080 1.47s/it\n",
            "Epoch: 481\n",
            "Train loss 481 0.929545 Grad Norm 2.146466 1.47s/it\n",
            "Epoch: 482\n",
            "Train loss 482 0.906083 Grad Norm 1.756446 1.70s/it\n",
            "Epoch: 483\n",
            "Train loss 483 0.928243 Grad Norm 2.742698 1.63s/it\n",
            "Epoch: 484\n",
            "Train loss 484 0.903295 Grad Norm 2.413018 1.52s/it\n",
            "Epoch: 485\n",
            "Train loss 485 0.929238 Grad Norm 2.275544 1.49s/it\n",
            "Epoch: 486\n",
            "Train loss 486 0.762339 Grad Norm 0.954225 1.73s/it\n",
            "Epoch: 487\n",
            "Train loss 487 0.804513 Grad Norm 2.547035 1.74s/it\n",
            "Epoch: 488\n",
            "Train loss 488 0.809546 Grad Norm 1.816686 1.87s/it\n",
            "Epoch: 489\n",
            "Train loss 489 0.955220 Grad Norm 3.382403 1.47s/it\n",
            "Epoch: 490\n",
            "Train loss 490 0.798847 Grad Norm 2.254442 1.73s/it\n",
            "Epoch: 491\n",
            "Train loss 491 0.779968 Grad Norm 1.252031 1.73s/it\n",
            "Epoch: 492\n",
            "Train loss 492 0.784532 Grad Norm 1.567998 1.73s/it\n",
            "Epoch: 493\n",
            "Train loss 493 0.899153 Grad Norm 2.017796 1.56s/it\n",
            "Epoch: 494\n",
            "Train loss 494 0.890982 Grad Norm 1.409426 1.48s/it\n",
            "Epoch: 495\n",
            "Train loss 495 0.906709 Grad Norm 1.550811 1.48s/it\n",
            "Epoch: 496\n",
            "Train loss 496 0.783921 Grad Norm 1.128049 1.73s/it\n",
            "Epoch: 497\n",
            "Train loss 497 0.749000 Grad Norm 0.908104 1.88s/it\n",
            "Epoch: 498\n",
            "Train loss 498 0.776257 Grad Norm 1.387843 1.87s/it\n",
            "Epoch: 499\n",
            "Train loss 499 0.763000 Grad Norm 0.951047 1.74s/it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r outdir /content/drive/MyDrive/Tacotron2_Sounds/\n"
      ],
      "metadata": {
        "id": "Zu7jo1fC-Q6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Tacotron2_Sounds/outdir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPDPF5ob-2FT",
        "outputId": "c594ef03-2bae-42c1-9adf-d1279329a312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_0  logdir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --checkpoint_file /content/drive/MyDrive/Tacotron2_Sounds/outdir/checkpoint_0 --input_text \"Merhaba, bu bir test metnidir.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iANF0uEj_BVU",
        "outputId": "ad81219f-0638-4f66-a33c-acb43a217605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/tacotron2/inference.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ]
}